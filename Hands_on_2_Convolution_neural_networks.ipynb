{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rania-azad/DL-Project1-10-07-2024/blob/main/Hands_on_2_Convolution_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🚀 Deep Learning Hands-On: Convolutional Neural Networks with MNIST in PyTorch\n",
        "\n",
        "Welcome to our exciting hands-on session on Deep Learning! In this workshop, we're going to dive into the fascinating world of Convolutional Neural Networks (CNNs) 🧠. Our focus will be on understanding and implementing CNNs using the famous MNIST dataset, a collection of handwritten digits that serves as a benchmark for image classification tasks.\n",
        "\n",
        "### What Will You Learn? 📚\n",
        "- **Basics of Convolutional Neural Networks:** Get to grips with the fundamental concepts of CNNs, including convolutional layers, pooling, and feature maps.\n",
        "- **PyTorch in Action:** Implement these concepts in PyTorch, one of the most popular frameworks for deep learning.\n",
        "- **Hands-On with MNIST:** Train a CNN on the MNIST dataset to classify handwritten digits.\n",
        "- **Visualization Techniques:** Learn how to visualize feature maps to understand what your CNN is learning.\n",
        "- **Model Evaluation:** We'll not only train our model but also evaluate its performance and learn how to fine-tune it for better results.\n",
        "\n",
        "### Let's Get Started! 🌟\n",
        "Prepare to embark on a journey through neural networks, where you'll get hands-on experience with training, visualizing, and evaluating your very own CNN model. Let's decode the mysteries of deep learning together!\n"
      ],
      "metadata": {
        "id": "b-LFHs4b_ikl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[[1, 2, 3]\n",
        " [4, 5, 6]]\n",
        "\n",
        "[1, 2, 3, 4, 5, 6]\n"
      ],
      "metadata": {
        "id": "9lkxp99rVC9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch                                    # PyTorch is a deep learning framework, torch is the library that trains and builds neural networks\n",
        "import torch.nn as nn                           # neural network layers + losses\n",
        "import torch.optim as optim                     # define optimizer (Stochastic Gradient Descent, Adam, RMSProp, Grad..)\n",
        "from torchvision import datasets, transforms    # standard datasets, transformations, standard models\n",
        "from torch.utils.data import DataLoader         # prepare data for training (split into batches)\n",
        "from torch.functional import F                  # defines functional layers"
      ],
      "metadata": {
        "id": "_BvE3pLYADOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Feature Learning</h2>\n",
        "\n",
        "Feature engineering is the process of extracting useful patterns from input data that will help the prediction model to understand better the real nature of the problem. A good feature learning will present patterns in a way that significantly increase the accuracy and performance of the applied machine learning algorithms in a way that would otherwise be impossible or too expensive by just machine learning itself.\n",
        "\n",
        "Feature learning algorithms finds the common patterns that are important to distinguish between the wanted classes and extract them automatically. After this process, they are ready to be used in a classification or regression problem.\n",
        "\n",
        "The great advantage of CNNs is that they are uncommonly good at finding features in images that grow after each level, resulting in high-level features in the end. The final layers (can be one or more) use all these generated features for classification or regression.\n",
        "\n",
        "Basically, Convolutional Neural Networks is your best friend to <b>automatically do Feature Engineering</b> (Feature Learning) without wasting too much time creating your own codes and with no prior need of expertise in the field of Feature Engineering.\n",
        "<br>\n",
        "\n",
        "<img src=\"https://ibm.box.com/shared/static/urzzkc7o5loqrlezcvn4kr594mxi9ftx.png\" alt=\"HTML5 Icon\" style=\"width: 650px; height: 250px;\">\n",
        "<center>\n",
        "    Example of feature learning (automatically feature engineering), starting with simple features and ending with high-level features like human faces. <a href=\"https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/\">ref</a>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "__ZUGGSgBk6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the MNIST Dataset 📚\n",
        "\n",
        "Before we dive into the architectural complexities of our CNN model, let's start with the fundamentals - loading and preprocessing our dataset. In this section, we will be working with the MNIST dataset, a cornerstone in the world of machine learning for image recognition tasks.\n"
      ],
      "metadata": {
        "id": "6jeH7skE_5TR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "042w9iAc_cMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5548543-c497-4c2c-bfdc-e56cc8495076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 55932152.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1966552.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 14078379.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2142193.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "# Torch contains all the necessary libraries for Deep Learning\n",
        "import torch\n",
        "# Torchvision contains different standard Datasets\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Transformations applied on each image\n",
        "# Transform is used to chain multiple transformation together, here we have:\n",
        "# convert image to pytorch/tensor type, and normalize the images to speed the process {computation and save memory},\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# TODO: Load the MNIST Desitnation of Download Destination -  download which type (train or test) - List of functions\n",
        "train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# TODO: Create data loaders - we do not shuffle in predicting next word or time series or sequences (order relation)\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Our Convolutional Neural Network (CNN) 🏗️\n",
        "\n",
        "With our MNIST dataset loaded and preprocessed, it's time to embark on the core journey of our hands-on session - developing a Convolutional Neural Network (CNN). In this section, we'll design and implement our CNN architecture using PyTorch.\n"
      ],
      "metadata": {
        "id": "l0RqAGZ8AM2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: complete this definition of CNN\n",
        "# A convolution layer in pytorch is defined with these parameters:\n",
        "# input channel (if RGB = 3, if grayscaled = 1), output channel (usually 16,32,64 ), kernel size (3,5,7), stride (use 1)\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        #Input_nb_channel, #output_channel, #kernel size\n",
        "        # Input size = 1, 28, 28\n",
        "        # formula to compute output = ((W-K+2P)/Stride) + 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3)  #  (28-3+0)/1 + 1 = 26\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3) #  (13-3+0)/1 + 1  = 11\n",
        "\n",
        "        # Output: (32, 5x5)\n",
        "        self.fc1 = nn.Linear(32*5*5,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))  #  (16, 26x26)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)  # add pooling layer  (pytorch doc is your friend) (16, 13x13)\n",
        "        x = F.relu(self.conv2(x)) #  (32, 11x11)\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)  # add pooling layer (32, 5x5)\n",
        "\n",
        "        x = x.view(-1,32*5*5)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def extract_features(self, x):\n",
        "        x1 = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x1, kernel_size=2, stride=2)\n",
        "        x2 = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x1, x2\n",
        "\n",
        "model = CNN()"
      ],
      "metadata": {
        "id": "D29y6KyVAHmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Convolutional Neural Network 🏋️‍♂️\n",
        "\n",
        "Now that our Convolutional Neural Network (CNN) is set up and ready, it's time to move on to one of the most crucial steps in the machine learning pipeline - training the model. In this section, we will train our CNN using the MNIST dataset, guiding it through the process of learning how to accurately classify handwritten digits.\n"
      ],
      "metadata": {
        "id": "-v96rf3xBtVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_accuracy = correct / total\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = correct / total\n",
        "        val_losses.append(val_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "id": "ILNQlueGBtDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# This function plots the model complexity graph\n",
        "def plot_model_complexity_graph(train_losses, val_losses, train_accuracies, val_accuracies):\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
        "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, train_accuracies, 'b-', label='Training Accuracy')\n",
        "    plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "noHMHK7cCM6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# sum(y - y') = y - (w1*x1 + w2*x2)\n",
        "#      dE/dw1 = x1\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# run the two functions\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
        "    # put here the arguments for the training function\n",
        "    model, train_loader, test_loader, criterion, optimizer, epochs=30\n",
        ")\n",
        "\n",
        "plot_model_complexity_graph(train_losses, val_losses, train_accuracies, val_accuracies)"
      ],
      "metadata": {
        "id": "5xfr4PAFCRwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "d860b92a-fb2b-4e91-c9f0-d83b5bd7ba76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-8013f9216f52>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# run the two functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# put here the arguments for the training function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b19bc3198e26>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iRROib3b_8ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: Comment on the model complexity graph and compare it with the fully connected one."
      ],
      "metadata": {
        "id": "mYZcEZhYCZN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BONUS PART: Visualizing the Learned Features of the CNN 🔍\n",
        "\n",
        "After training our Convolutional Neural Network on the MNIST dataset, an intriguing question arises: \"What has our network learned?\" In this section, we aim to shed light on this by visualizing the features extracted by the CNN's convolutional layers.\n",
        "\n",
        "### Why Visualize Features? 🤔\n",
        "- **Understanding the Model:** Visualization helps us understand what features the network finds significant in the input data.\n",
        "- **Debugging the Model:** If the model is not performing as expected, examining these visualizations can help identify what the model is focusing on and where it might be going wrong.\n",
        "- **Enhancing Intuition:** For educational purposes, these visualizations can greatly enhance our intuition about how convolutional networks process and interpret image data."
      ],
      "metadata": {
        "id": "sYT1YwNlCiH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_feature_maps(model, image, layer_num):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        conv1_output, conv2_output = model.extract_features(image.unsqueeze(0))\n",
        "\n",
        "    if layer_num == 1:\n",
        "        layer_output = conv1_output\n",
        "    elif layer_num == 2:\n",
        "        layer_output = conv2_output\n",
        "    else:\n",
        "        raise ValueError(\"Invalid layer number\")\n",
        "\n",
        "    layer_output = layer_output.squeeze(0).cpu()\n",
        "\n",
        "    fig, axes = plt.subplots(1, layer_output.size(0), figsize=(20, 2))\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.imshow(layer_output[i].detach().numpy(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d7742VBrCegn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: try different images and compare the result\n",
        "# Load a sample image from the dataset\n",
        "sample_image, _ = next(iter(test_loader))\n",
        "sample_image = sample_image[3]  # Get the first image in the batch\n",
        "\n",
        "# Visualize feature maps for the first convolutional layer\n",
        "visualize_feature_maps(model, sample_image, layer_num=1)\n",
        "\n",
        "# Visualize feature maps for the second convolutional layer\n",
        "visualize_feature_maps(model, sample_image, layer_num=2)"
      ],
      "metadata": {
        "id": "A-OLcs82CuSF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "8a591663-d97d-42f2-9d06-76ebcdad0e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CNN' object has no attribute 'extract_features'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-af5f7f52ddac>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Visualize feature maps for the first convolutional layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvisualize_feature_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Visualize feature maps for the second convolutional layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bf3dc6a004bd>\u001b[0m in \u001b[0;36mvisualize_feature_maps\u001b[0;34m(model, image, layer_num)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mconv1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv2_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlayer_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CNN' object has no attribute 'extract_features'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TODO: comment the feature maps\n",
        "### TODO: Redo the full thing with a deeper network and compare."
      ],
      "metadata": {
        "id": "iahOfS2PDDq2"
      }
    }
  ]
}